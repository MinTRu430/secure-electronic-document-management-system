from __future__ import annotations

import json
from typing import Any, Dict, List, Tuple

from app.db import get_conn

ALLOWED_TYPES = {
    "integer": "INTEGER",
    "bigint": "BIGINT",
    "varchar": "VARCHAR",
    "text": "TEXT",
    "bool": "BOOLEAN",
    "timestamp": "TIMESTAMPTZ",
    "date": "DATE",
    "bytea": "BYTEA",
}


def _ident(name: str) -> str:
    if not name or not name.replace("_", "").isalnum():
        raise ValueError(f"Bad identifier: {name}")
    return name


def create_table(payload: Dict[str, Any], schema: str = "public") -> Dict[str, Any]:
    if schema != "public":
        raise ValueError("Only public schema allowed")

    table = _ident(payload["table"])
    columns: List[Dict[str, Any]] = payload.get("columns", [])
    pk: List[str] = payload.get("primary_key", [])
    uniques: List[List[str]] = payload.get("uniques", [])
    fks: List[Dict[str, str]] = payload.get("foreign_keys", [])

    if not columns:
        raise ValueError("columns must not be empty")

    pk_set = {_ident(x) for x in pk}

    col_defs: List[str] = []
    # logical file-columns metadata (inline storage)
    file_columns: List[Dict[str, Any]] = []
    # SQL comments we will apply after CREATE TABLE
    comment_sqls: List[Tuple[str, str]] = []  # (column_name, comment_json)

    for c in columns:
        base = _ident(c["name"])

        # ===== FILE COLUMN (INLINE) =====
        # When column has "file" metadata, we create TWO physical columns:
        #   <base>_name (TEXT) and <base>_data (TEXT or BYTEA depending on mode)
        if "file" in c:
            f = c["file"]
            storage_mode = f.get("storage_mode")
            required = bool(f.get("required", False))
            if storage_mode not in ("base64", "blob", "fs"):
                raise ValueError("file.storage_mode must be base64, blob or fs")

            name_col = _ident(f"{base}_name")
            data_col = _ident(f"{base}_data")

            # For base64/fs we store TEXT, for blob we store BYTEA
            data_type = "TEXT" if storage_mode in ("base64", "fs") else "BYTEA"

            # required => both NOT NULL
            nn = "NOT NULL" if required else ""
            col_defs.append(f'"{name_col}" TEXT {nn}'.strip())
            col_defs.append(f'"{data_col}" {data_type} {nn}'.strip())

            # Remember file meta for UI (via schema introspection)
            meta = {
                "file": True,
                "base": base,
                "name_col": name_col,
                "mode": storage_mode,
                "required": required,
            }
            comment_sqls.append((data_col, json.dumps(meta, ensure_ascii=False)))

            file_columns.append({
                "base": base,
                "name_column": name_col,
                "data_column": data_col,
                "storage_mode": storage_mode,
                "required": required,
            })
            continue

        # ===== NORMAL COLUMN =====
        t = c["type"]
        t_sql = ALLOWED_TYPES.get(t)
        if not t_sql:
            raise ValueError(f"Type not allowed: {t}")

        if t == "varchar":
            n = int(c.get("length", 200))
            if n < 1 or n > 2000:
                raise ValueError("varchar length out of range")
            t_sql = f"VARCHAR({n})"

        nullable = bool(c.get("nullable", True))
        unique = bool(c.get("unique", False))
        default = c.get("default")

        parts = [f'"{base}"', t_sql]

        # ✅ АВТО-ID: integer/bigint + PK → IDENTITY
        if base in pk_set and t in ("integer", "bigint") and not default:
            parts.append("GENERATED BY DEFAULT AS IDENTITY")

        if not nullable or base in pk_set:
            parts.append("NOT NULL")
        if unique:
            parts.append("UNIQUE")
        if default:
            parts.append(f"DEFAULT {default}")

        col_defs.append(" ".join(parts))

    constraints: List[str] = []

    if pk:
        pk_cols = [f'"{_ident(x)}"' for x in pk]
        constraints.append(f"PRIMARY KEY ({', '.join(pk_cols)})")

    for u in uniques:
        u_cols = [f'"{_ident(x)}"' for x in u]
        constraints.append(f"UNIQUE ({', '.join(u_cols)})")

    for fk in fks:
        col = _ident(fk["column"])
        ref_table = _ident(fk["ref_table"])
        ref_col = _ident(fk.get("ref_column", "id"))
        on_delete = fk.get("on_delete", "CASCADE")
        on_update = fk.get("on_update", "CASCADE")

        constraints.append(
            f'FOREIGN KEY ("{col}") REFERENCES "{schema}"."{ref_table}"("{ref_col}") '
            f"ON UPDATE {on_update} ON DELETE {on_delete}"
        )

    create_sql = (
        f'CREATE TABLE "{schema}"."{table}" (\n  '
        + ",\n  ".join(col_defs + constraints)
        + "\n);"
    )

    with get_conn() as conn, conn.cursor() as cur:
        cur.execute(create_sql)

        # Attach COMMENT metadata for inline file columns
        for col_name, comment in comment_sqls:
            cur.execute(
                f'COMMENT ON COLUMN "{schema}"."{table}"."{col_name}" IS %s;',
                (comment,),
            )

    return {
        "created": True,
        "table": table,
        "file_columns": file_columns,
        "sql": create_sql,
    }
